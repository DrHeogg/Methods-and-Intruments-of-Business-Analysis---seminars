{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "914eceb1",
   "metadata": {},
   "source": [
    "# Семинар 2. Практическая часть №5\n",
    "## Обработка текстов и анализ в стиле Orange (Google Colab)\n",
    "\n",
    "**Цели работы:**\n",
    "- осознать аналогию «аддонов» Orange и дополнительных библиотек Python;\n",
    "- освоить базовую предварительную обработку текста (очистка, токенизация, стоп-слова, лемматизация);\n",
    "- построить облако слов по корпусу текстов;\n",
    "- выполнить кластеризацию текстов;\n",
    "- обучить простую модель классификации текстов и оценить её качество."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a92691e",
   "metadata": {},
   "source": [
    "## Блок 0. Установка и импорт библиотек (аналог аддонов)\n",
    "\n",
    "В Orange мы подключаем аддоны через интерфейс.\n",
    "В Google Colab аналогом будет установка дополнительных пакетов (`nltk`, `wordcloud`) и импорт нужных модулей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb075d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка дополнительных библиотек (аналог установки аддонов в Orange)\n",
    "!pip install nltk wordcloud -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5780955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups, load_iris\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 20)\n",
    "pd.set_option(\"display.width\", 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ee3a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка ресурсов NLTK для токенизации, стоп-слов и лемматизации\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a71a122",
   "metadata": {},
   "source": [
    "### Мини-пример: MDS для числовых данных\n",
    "\n",
    "В качестве аналога нового функционала (как аддон с MDS в Orange)\n",
    "возьмём числовой датасет Iris и понизим размерность методом `MDS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f3786e",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "target_names_iris = iris.target_names\n",
    "\n",
    "mds_num = MDS(n_components=2, random_state=42)\n",
    "X_iris_2d = mds_num.fit_transform(X_iris)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X_iris_2d[:, 0], X_iris_2d[:, 1], c=y_iris)\n",
    "plt.title(\"MDS-проекция датасета Iris\")\n",
    "plt.xlabel(\"Компонента 1\")\n",
    "plt.ylabel(\"Компонента 2\")\n",
    "plt.legend(handles=scatter.legend_elements()[0],\n",
    "           labels=target_names_iris,\n",
    "           title=\"Вид\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab487d0f",
   "metadata": {},
   "source": [
    "## Блок 1. Загрузка корпуса текстов\n",
    "\n",
    "В Orange используется встроенный корпус Grimm tales.\n",
    "Здесь возьмём корпус новостных текстов `20 newsgroups` из `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288ff037",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"sci.space\", \"rec.sport.baseball\", \"talk.politics.mideast\"]\n",
    "\n",
    "newsgroups = fetch_20newsgroups(\n",
    "    subset=\"train\",\n",
    "    categories=categories,\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "texts = newsgroups.data\n",
    "labels = newsgroups.target\n",
    "target_names = newsgroups.target_names\n",
    "\n",
    "print(f\"Количество документов: {len(texts)}\")\n",
    "print(\"Категории:\", target_names)\n",
    "print(\"\\nФрагмент первого документа:\\n\")\n",
    "print(texts[0][:700])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6186ad1",
   "metadata": {},
   "source": [
    "## Блок 2. Предварительная обработка текста (№16)\n",
    "\n",
    "Аналог виджета **Preprocess Text** в Orange: очистка, токенизация, стоп-слова, лемматизация."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf0c2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if len(t) > 2 and t not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "raw_example = texts[0]\n",
    "clean_example = preprocess_text(raw_example)\n",
    "\n",
    "print(\"=== ОРИГИНАЛ ===\")\n",
    "print(raw_example[:500])\n",
    "print(\"\\n=== ПОСЛЕ ПРЕДОБРАБОТКИ ===\")\n",
    "print(clean_example[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e5c825",
   "metadata": {},
   "source": [
    "## Блок 3. Массовая предобработка и облако слов\n",
    "\n",
    "Применяем функцию предобработки ко всем документам\n",
    "и строим облако слов по всему корпусу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c6d2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_texts = [preprocess_text(t) for t in texts]\n",
    "\n",
    "len(preprocessed_texts), preprocessed_texts[0][:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61294ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \" \".join(preprocessed_texts)\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400).generate(all_text)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Облако слов по корпусу (после предобработки)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def6e4ff",
   "metadata": {},
   "source": [
    "## Блок 4. Кластеризация текстов (№17)\n",
    "\n",
    "TF-IDF векторизация, косинусные расстояния, MDS-проекция и иерархическая кластеризация."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a33446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=2000)\n",
    "X = vectorizer.fit_transform(preprocessed_texts)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecddb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = cosine_distances(X)\n",
    "dist_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662dd5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mds = MDS(\n",
    "    n_components=2,\n",
    "    dissimilarity=\"precomputed\",\n",
    "    random_state=42,\n",
    "    n_init=4,\n",
    "    max_iter=300,\n",
    ")\n",
    "X_2d = mds.fit_transform(dist_matrix)\n",
    "\n",
    "X_2d[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce01abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 3\n",
    "\n",
    "cluster_model = AgglomerativeClustering(\n",
    "    n_clusters=n_clusters,\n",
    "    affinity=\"precomputed\",\n",
    "    linkage=\"average\",\n",
    ")\n",
    "cluster_labels = cluster_model.fit_predict(dist_matrix)\n",
    "\n",
    "np.bincount(cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d124df63",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "for cluster_id in range(n_clusters):\n",
    "    mask = cluster_labels == cluster_id\n",
    "    plt.scatter(\n",
    "        X_2d[mask, 0],\n",
    "        X_2d[mask, 1],\n",
    "        label=f\"cluster {cluster_id}\",\n",
    "        alpha=0.6,\n",
    "    )\n",
    "plt.title(\"MDS-проекция TF-IDF и кластеры (Agglomerative)\")\n",
    "plt.xlabel(\"Компонента 1\")\n",
    "plt.ylabel(\"Компонента 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4f9f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_examples_for_cluster(cluster_id: int, n_examples: int = 3):\n",
    "    print(f\"\\n=== Кластер {cluster_id} ===\")\n",
    "    idxs = np.where(cluster_labels == cluster_id)[0][:n_examples]\n",
    "    for idx in idxs:\n",
    "        true_label = target_names[labels[idx]]\n",
    "        print(f\"\\n--- Документ {idx}, истинная категория: {true_label}\")\n",
    "        print(texts[idx][:400], \"...\\n\")\n",
    "\n",
    "for cid in range(n_clusters):\n",
    "    print_examples_for_cluster(cid, n_examples=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc66012a",
   "metadata": {},
   "source": [
    "## Блок 5. Классификация текстов (№18)\n",
    "\n",
    "Строим конвейер `TF-IDF + Logistic Regression`, обучаем и оцениваем качество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932f3ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=labels,\n",
    ")\n",
    "\n",
    "clf_pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"tfidf\",\n",
    "            TfidfVectorizer(\n",
    "                max_features=5000,\n",
    "                stop_words=\"english\",\n",
    "                lowercase=True,\n",
    "            ),\n",
    "        ),\n",
    "        (\"clf\", LogisticRegression(max_iter=1000)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "clf_pipeline.fit(X_train, y_train)\n",
    "y_pred = clf_pipeline.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895deb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "im = ax.imshow(cm)\n",
    "\n",
    "ax.set_xticks(range(len(target_names)))\n",
    "ax.set_yticks(range(len(target_names)))\n",
    "ax.set_xticklabels(target_names, rotation=45, ha=\"right\")\n",
    "ax.set_yticklabels(target_names)\n",
    "\n",
    "ax.set_xlabel(\"Предсказанный класс\")\n",
    "ax.set_ylabel(\"Истинный класс\")\n",
    "ax.set_title(\"Матрица ошибок\")\n",
    "\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a726461",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"NASA announced a new mission to Mars with a powerful new rocket.\"\n",
    "pred_label = clf_pipeline.predict([example_text])[0]\n",
    "\n",
    "print(\"Текст:\", example_text)\n",
    "print(\"Предсказанная категория:\", target_names[pred_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f8c0c6",
   "metadata": {},
   "source": [
    "## Блок 6. Выводы\n",
    "\n",
    "В ходе практической работы:\n",
    "\n",
    "- показан аналог «аддонов» Orange через установку и использование дополнительных библиотек в Google Colab;\n",
    "- выполнена предобработка текстов (очистка, токенизация, стоп-слова, лемматизация);\n",
    "- построено облако слов по корпусу документов;\n",
    "- проведена кластеризация текстов на основе TF-IDF-признаков и MDS-проекции;\n",
    "- обучена и оценена простая модель классификации текстов по тематикам.\n",
    "\n",
    "Таким образом, задания Практической части №5 реализованы в среде Google Colab с сохранением логики оригинальных упражнений."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
